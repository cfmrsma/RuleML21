{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "warming-hampton",
   "metadata": {},
   "source": [
    "# Essai perceptron multi-couche sur données miniloan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-energy",
   "metadata": {},
   "source": [
    "## Acquisition des données d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subjective-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb instances =  10001  X =  [['Revenu', 'Credit score', 'Montant', 'DurÃ©e', 'Taux'], ['307666', '197', '217370', '21', '0.043196103380325666'], ['416779', '947', '1015923', '7', '0.05206380610252473'], ['110310', '162', '259208', '4', '0.04068419371946227'], ['214882', '440', '267647', '13', '0.0411346949665408']]  Y =  ['Status', '0', '0', '0', '1']\n",
      "Nb instances positives =  4720 , nb instances négatives =  5280\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import csv;\n",
    "f= open (\"../miniloan_csma_10000.csv\", 'r')\n",
    "myReader = csv.reader(f)\n",
    "f.close\n",
    "X_train_origine=list()\n",
    "y_train_origine=list()\n",
    "\n",
    "for row in myReader:\n",
    "    X_train_origine.append(row[2:4]+row[5:8])\n",
    "    y_train_origine.append(row[8])\n",
    "\n",
    "#X_train=np.array(X_train)\n",
    "print(\"Nb instances = \", len(X_train_origine),\" X = \", X_train_origine[0:5], \" Y = \",y_train_origine[0:5])\n",
    "X_train_origine=X_train_origine[1:]\n",
    "y_train_origine=y_train_origine[1:]\n",
    "print(\"Nb instances positives = \",y_train_origine.count('1'),\", nb instances négatives = \",y_train_origine.count('0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-drive",
   "metadata": {},
   "source": [
    "## Acquisition des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "union-vinyl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [['Revenu', 'Credit score', 'Montant', 'DurÃ©e', 'Taux'], ['502389', '150', '476355', '25', '0.04171977146573331'], ['543158', '972', '577534', '14', '0.027923264583234676'], ['464520', '928', '85594', '3', '0.04876627034264725'], ['493692', '7', '1117942', '18', '0.03615719567808266']]  Y =  ['Status', '0', '1', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "f= open (\"../miniloan_csma_1000.csv\", 'r')\n",
    "myReader = csv.reader(f)\n",
    "f.close\n",
    "X_test_origine=list()\n",
    "y_test_origine=list()\n",
    "\n",
    "for row in myReader:\n",
    "    X_test_origine.append(row[2:4]+row[5:8])\n",
    "    y_test_origine.append(row[8])\n",
    "\n",
    "print(\"X = \", X_test_origine[0:5], \" Y = \",y_test_origine[0:5])\n",
    "X_test_origine=X_test_origine[1:]\n",
    "y_test_origine=y_test_origine[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-vessel",
   "metadata": {},
   "source": [
    "## Normalisation des données d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continental-compound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X normalisé =  [[ 0.0348213  -1.0387978  -1.14822718  0.62903175  0.30249088]\n",
      " [ 0.66608223  1.55517127  1.18756092 -0.99691347  0.77543804]\n",
      " [-1.10695955 -1.15984969 -1.02584995 -1.34533031  0.16852151]\n",
      " [-0.50197006 -0.19835182 -1.00116566 -0.30007981  0.1925484 ]\n",
      " [-1.16063059 -1.12872206  1.63347232  1.32586542 -0.04632843]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X_train_origine)  \n",
    "X_train = scaler.transform(X_train_origine)  \n",
    "# apply same transformation to test data\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"X normalisé = \", X_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-delight",
   "metadata": {},
   "source": [
    "## Normalisation des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coral-procurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X normalisé =  [[ 1.16136922 -1.20135319 -0.39068938  1.09358753  0.22375268]\n",
      " [ 1.39723367  1.6416369  -0.0947382  -0.18394086 -0.51206571]\n",
      " [ 0.9422824   1.48945738 -1.53367538 -1.46146925  0.59956834]\n",
      " [ 1.11105371 -1.69593663  1.48596912  0.28061492 -0.07291992]\n",
      " [-0.21736373  0.13713484 -0.90460764 -1.34533031 -1.71571247]]\n"
     ]
    }
   ],
   "source": [
    "# apply same normalisation to test data\n",
    "\n",
    "X_test = scaler.transform(X_test_origine)\n",
    "\n",
    "print(\"X normalisé = \", X_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-direction",
   "metadata": {},
   "source": [
    "## Redéfinition de la fonction forward_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scheduled-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as logistic_sigmoid\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "def my_inplace_logistic(X):\n",
    "    \"\"\"Compute the logistic function inplace. From sklearn/NN/_base\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    \"\"\"\n",
    "    logistic_sigmoid(X, out=X)\n",
    "\n",
    "def my_forward_pass(mlp, instance):\n",
    "        \"\"\"Perform a forward pass on the network by computing the values\n",
    "        of the neurons in the hidden layers and the output layer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        activations : list, length = n_layers - 1\n",
    "            The ith element of the list holds the values of the ith layer.\n",
    "        \"\"\"\n",
    "        activations=[np.array(instance)]\n",
    "#        hidden_activation = ACTIVATIONS[mlp.activation]\n",
    "        # Iterate over the hidden layers\n",
    "        for i in range(mlp.n_layers_ - 1):\n",
    "            activations.append(safe_sparse_dot(activations[i], mlp.coefs_[i]))\n",
    "            activations[i + 1] += mlp.intercepts_[i]\n",
    "\n",
    "            # For the hidden layers\n",
    "            if (i + 1) != (mlp.n_layers_ - 1):\n",
    "                my_inplace_logistic(activations[i + 1])\n",
    "#                hidden_activation(activations[i + 1])\n",
    "\n",
    "        # For the last layer\n",
    "#        output_activation = ACTIVATIONS[mlp.out_activation_]\n",
    "        my_inplace_logistic(activations[i + 1])\n",
    "#        output_activation(activations[i + 1])\n",
    "\n",
    "        return activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-ready",
   "metadata": {},
   "source": [
    "## Création, paramétrage et entraînement de l'arbre de décision et du réseau sur les données d'origine et de l'arbre de décision sur les données tranformées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle  0\n",
      "Nb instances =  1000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  37  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.953\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  1.0 Score sur les données de test =  0.97\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  4  Score sur les données d'apprentissage =  0.995004995004995  Score sur les données de test =  0.972\n",
      "Nb instances =  2000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  57  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.961\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  0.9895052473763118 Score sur les données de test =  0.972\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  3  Score sur les données d'apprentissage =  0.960519740129935  Score sur les données de test =  0.949\n",
      "Nb instances =  3000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  80  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.964\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  1.0 Score sur les données de test =  0.989\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  3  Score sur les données d'apprentissage =  0.9976674441852715  Score sur les données de test =  0.988\n",
      "Nb instances =  4000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  88  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.968\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  0.9455136215946014 Score sur les données de test =  0.946\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  2  Score sur les données d'apprentissage =  0.8887778055486129  Score sur les données de test =  0.889\n",
      "Nb instances =  5000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  103  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.966\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  1.0 Score sur les données de test =  0.997\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  5  Score sur les données d'apprentissage =  0.9974005198960207  Score sur les données de test =  0.995\n",
      "Nb instances =  6000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  121  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.974\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  0.988835194134311 Score sur les données de test =  0.991\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  2  Score sur les données d'apprentissage =  0.9601733044492584  Score sur les données de test =  0.958\n",
      "Nb instances =  7000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  141  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChristiandeSainteMar\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réseau de neurones : Score sur les données d'apprentissage =  0.9728610198543065 Score sur les données de test =  0.977\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  11  Score sur les données d'apprentissage =  0.9617197543208114  Score sur les données de test =  0.966\n",
      "Nb instances =  8000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  148  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.978\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  0.9993750781152356 Score sur les données de test =  0.996\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  3  Score sur les données d'apprentissage =  0.9900012498437696  Score sur les données de test =  0.99\n",
      "Nb instances =  9000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  157  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChristiandeSainteMar\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réseau de neurones : Score sur les données d'apprentissage =  0.998555716031552 Score sur les données de test =  0.994\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  9  Score sur les données d'apprentissage =  0.9918897900233308  Score sur les données de test =  0.988\n",
      "Nb instances =  10000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  162  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.981\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  0.9449 Score sur les données de test =  0.94\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  9  Score sur les données d'apprentissage =  0.9421  Score sur les données de test =  0.944\n",
      "Shuffle  1\n",
      "Nb instances =  1000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  43  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.965\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  1.0 Score sur les données de test =  0.982\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  2  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.983\n",
      "Nb instances =  2000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  68  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChristiandeSainteMar\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réseau de neurones : Score sur les données d'apprentissage =  1.0 Score sur les données de test =  0.98\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  3  Score sur les données d'apprentissage =  0.9780109945027486  Score sur les données de test =  0.978\n",
      "Nb instances =  3000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  75  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.97\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  1.0 Score sur les données de test =  0.992\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  3  Score sur les données d'apprentissage =  0.9946684438520493  Score sur les données de test =  0.99\n",
      "Nb instances =  4000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  101  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChristiandeSainteMar\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réseau de neurones : Score sur les données d'apprentissage =  0.9385153711572107 Score sur les données de test =  0.934\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  10  Score sur les données d'apprentissage =  0.9042739315171208  Score sur les données de test =  0.893\n",
      "Nb instances =  5000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  114  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.969\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  0.9986002799440112 Score sur les données de test =  0.998\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  2  Score sur les données d'apprentissage =  0.9926014797040592  Score sur les données de test =  0.996\n",
      "Nb instances =  6000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  124  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.974\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  0.9995000833194467 Score sur les données de test =  0.997\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  5  Score sur les données d'apprentissage =  0.9936677220463256  Score sur les données de test =  0.994\n",
      "Nb instances =  7000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  125  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.978\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  0.9987144693615198 Score sur les données de test =  0.996\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  5  Score sur les données d'apprentissage =  0.9958577346093416  Score sur les données de test =  0.994\n",
      "Nb instances =  8000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  140  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.976\n",
      "Réseau de neurones : Score sur les données d'apprentissage =  0.9388826396700413 Score sur les données de test =  0.935\n",
      "Arbre de décision données transformées : Nb feuilles (= règles) :  6  Score sur les données d'apprentissage =  0.8578927634045744  Score sur les données de test =  0.847\n",
      "Nb instances =  9000\n",
      "Arbre de décision données d'origine : Nb feuilles (= règles) :  147  Score sur les données d'apprentissage =  1.0  Score sur les données de test =  0.969\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTree_origine = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 20)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=0.0000000001, hidden_layer_sizes=(5,5,3), random_state=1, max_iter=200000, activation='logistic')\n",
    "decisionTree_meta = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 10)\n",
    "\n",
    "X_test_meta = np.empty((len(X_test),clf.hidden_layer_sizes[-1]))\n",
    "\n",
    "out_data = np.empty((10, 10, 5))\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "for s in range(10):\n",
    "    X_train_origine, X_train, y_train_origine = shuffle(X_train_origine, X_train, y_train_origine)\n",
    "    print(\"Shuffle \",s)\n",
    "    \n",
    "    inst_idx =0\n",
    "    for nbi in range(1000,10001,1000):\n",
    "        print(\"Nb instances = \",nbi)\n",
    "\n",
    "        #Construction et évaluation de l'arbre de décision\n",
    "        decisionTree_origine.fit(X_train_origine[0:nbi+1],y_train_origine[0:nbi+1])\n",
    "        nbRegles_origine = decisionTree_origine.get_n_leaves()\n",
    "        DTscoreTest_origine = decisionTree_origine.score(X_test_origine, y_test_origine)\n",
    "        print(\"Arbre de décision données d'origine : Nb feuilles (= règles) : \",nbRegles_origine,\" Score sur les données d'apprentissage = \", decisionTree_origine.score(X_train_origine[0:nbi+1],y_train_origine[0:nbi+1]),\" Score sur les données de test = \", DTscoreTest_origine)\n",
    "\n",
    "        # Construction et évaluation du réseau de neurones\n",
    "        clf.fit(X_train[0:nbi+1],y_train_origine[0:nbi+1])\n",
    "        NNscoreTest = clf.score(X_test, y_test_origine)\n",
    "        print(\"Réseau de neurones : Score sur les données d'apprentissage = \", clf.score(X_train[0:nbi+1],y_train_origine[0:nbi+1]),\"Score sur les données de test = \", NNscoreTest)\n",
    "\n",
    "        # Récupération de la dernière couche du réseau et transformation des données d'entrainement\n",
    "        X_train_meta = np.empty((len(X_train[0:nbi+1]),clf.hidden_layer_sizes[-1]))\n",
    "\n",
    "        for i in range(len(X_train[0:nbi+1])):\n",
    "            brut = my_forward_pass(clf,X_train[i])[-2]\n",
    "            j=0\n",
    "            for x in brut:\n",
    "                X_train_meta[i][j] = 0.00 if x <= 0.33 else 0.5 if x < 0.67 else 1\n",
    "                j=j+1\n",
    "\n",
    "        # Récupération de la dernière couche du réseau et transformation des données de test\n",
    "        for i in range(len(X_test)):\n",
    "            brut = my_forward_pass(clf,X_test[i])[-2]\n",
    "            j=0\n",
    "            for x in brut:\n",
    "                X_test_meta[i][j] = 0.00 if x <= 0.33 else 0.5 if x < 0.67 else 1\n",
    "                j=j+1\n",
    "\n",
    "        # Construction et évaluation d'un arbre de décision construit sur les données transformées\n",
    "        decisionTree_meta.fit(X_train_meta,y_train_origine[0:nbi+1])\n",
    "        nbRegles_meta = decisionTree_meta.get_n_leaves()\n",
    "        DTscoreTest_meta = decisionTree_meta.score(X_test_meta, y_test_origine)\n",
    "        print(\"Arbre de décision données transformées : Nb feuilles (= règles) : \",nbRegles_meta,\" Score sur les données d'apprentissage = \", decisionTree_meta.score(X_train_meta, y_train_origine[0:nbi+1]),\" Score sur les données de test = \", DTscoreTest_meta)\n",
    "        \n",
    "        out_data[s,inst_idx] = [nbRegles_origine,DTscoreTest_origine,nbRegles_meta,DTscoreTest_meta,NNscoreTest]\n",
    "        inst_idx = inst_idx+1\n",
    "\n",
    "out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the results array: \",out_data.shape)\n",
    "\n",
    "print(\"Dimensions of the results array: \",out_data.ndim)\n",
    "\n",
    "shuffle_Means = out_data.mean(axis=0)\n",
    "shuffle_Std = out_data.std(axis=0)\n",
    "print(\"Mean of results array - Along axis Shuffle: \",shuffle_Means)\n",
    "print(\"Standard deviation of results array - Along axis Shuffle: \",shuffle_Std)\n",
    "\n",
    "#print(\"Mean of results array - Along axis Nb Instances: \",out_data.mean(axis=1))\n",
    "#print(\"Mean of results array - Along axis Values : \",out_data.mean(axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, constrained_layout=True)#nrows=2)\n",
    "#plt.title('Average number of rules with growing number of training instances')\n",
    "ax0.errorbar(list(range(1000,10001,1000)), shuffle_Means[:,0], yerr=shuffle_Std[:,0], fmt='-o')\n",
    "ax1.errorbar(list(range(1000,10001,1000)), shuffle_Means[:,2], yerr=shuffle_Std[:,2], fmt='-o')\n",
    "ax0.set_title(\"(a)\")\n",
    "ax0.set_xlabel(\"Nb training instances\")\n",
    "ax0.set_ylabel(\"Nb rules\")\n",
    "ax1.set_title(\"(b)\")\n",
    "ax1.set_xlabel(\"Nb training instances\")\n",
    "#ax1.set_ylabel(\"Nb rules\")\n",
    "fig.set_size_inches(15,3)\n",
    "plt.savefig(\"fig-5-5-3.png\",format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
